{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Information Retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import ParserCACM\n",
    "import TextRepresenter\n",
    "import indexation\n",
    "import modeles\n",
    "from query import QueryParserCACM\n",
    "import evaluation\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import operator\n",
    "# Auto reload the imported modules when running cells\n",
    "%load_ext autoreload \n",
    "%autoreload 2\n",
    "\n",
    "# Constants:\n",
    "srcFolder = \"cacm/\" \n",
    "srcFile = \"cacm.txt\"\n",
    "qryFile = \"cacm.qry\"\n",
    "relFile = \"cacm.rel\"\n",
    "gendata = \"gendata\" #output folder\n",
    "indexName = \"cacm\"\n",
    "\n",
    "\n",
    "cacm_txt = os.path.join(srcFolder, srcFile)\n",
    "cacm_qry = os.path.join(srcFolder, qryFile)\n",
    "cacm_rel = os.path.join(srcFolder, relFile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Testing ParserCACM:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##### Testing ParserCACM #####\n",
      "Doc #1 is as expected\n",
      "Doc #178 is as expected\n",
      "Doc #3204 is as expected\n",
      "Success: there are 3205 documents as expected\n",
      "##### Test: success #####\n"
     ]
    }
   ],
   "source": [
    "print(\"##### Testing ParserCACM #####\")\n",
    "parser = ParserCACM.ParserCACM()\n",
    "parser.initFile(cacm_txt)\n",
    "doc = parser.nextDocument()\n",
    "noErrors = True\n",
    "nbDocs = 3205\n",
    "i = 1\n",
    "while doc is not None:\n",
    "    docId = int(doc.getId())\n",
    "    docTxt = doc.getText()\n",
    "    if docId != i:\n",
    "        noErrors = False\n",
    "        print(\"Error, doc at position %d is #%d\" % (i, docId))\n",
    "    if docId == 1:\n",
    "        if (\"Preliminary Report-International Algebraic Language\" not in docTxt and\n",
    "        \"Perlis\" not in docTxt and \n",
    "        \"Samelson\" not in docTxt):\n",
    "            print(\"Error, the document #1 is not as expected.\")\n",
    "        else:\n",
    "            print(\"Doc #1 is as expected\")\n",
    "        \n",
    "    elif docId == 178:\n",
    "        if (\"ROOTFINDER\" not in docTxt and\n",
    "        \"Thacher\" not in docTxt):\n",
    "            print(\"Error, the document #178 is not as expected.\")\n",
    "        else:\n",
    "            print(\"Doc #178 is as expected\")        \n",
    "            \n",
    "    elif docId == 3204:\n",
    "        if (\"An On-Line Program for Non-Numerical Algebra\" not in docTxt and\n",
    "        \"The goal of this program is\" not in docTxt and\n",
    "        \"console at Stanford University.\" not in docTxt):\n",
    "            print(\"Error, the document #3204 is not as expected.\")\n",
    "        else:\n",
    "            print(\"Doc #3204 is as expected\")        \n",
    "        \n",
    "    i += 1\n",
    "    doc = parser.nextDocument()\n",
    "    \n",
    "if i == nbDocs:\n",
    "    print(\"Success: there are %d documents as expected\" % nbDocs)\n",
    "else:\n",
    "    print(\"Error, found %d docs, should contain %d docs\" % (i, nbDocs))\n",
    "    noErrors = False\n",
    "\n",
    "if noErrors:\n",
    "    print(\"##### Test: success #####\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Testing PorterStemmer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##### Testing PorterStemmer start    #####\n",
      "Stemmer works as expected\n",
      "##### Testing PorterStemmer finished #####\n"
     ]
    }
   ],
   "source": [
    "print(\"##### Testing PorterStemmer start    #####\")\n",
    "stemmer = TextRepresenter.PorterStemmer()\n",
    "txtRepr = (stemmer.getTextRepresentation(\"Information retrieval (IR) is the activity of \\\n",
    "    obtaining information resources relevant to an \\\n",
    "    information need from a collection of information resources\"))\n",
    "expectDic = {'resourc':2, 'inform':4, 'relev':1,\n",
    "             'retriev':1, 'activ': 1, 'obtain':1,\n",
    "             'collect':1, 'ir':1}\n",
    "if txtRepr != expectDic:\n",
    "    print(\"Error, stemmer does not work as expected\")\n",
    "else:\n",
    "    print(\"Stemmer works as expected\")\n",
    "print(\"##### Testing PorterStemmer finished #####\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Testing the indexation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing the indexation...\n",
      "1st pass: build the index...\n",
      "2nd pass: build the inverted index...\n",
      "Finished.\n",
      "CPU times: user 5.28 s, sys: 556 ms, total: 5.84 s\n",
      "Wall time: 7.12 s\n"
     ]
    }
   ],
   "source": [
    "# Construct the index:\n",
    "idx = indexation.Index(indexName, gendata)\n",
    "%time idx.indexation(cacm_txt, parser, stemmer, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "###### Testing the index: ###### \n",
      "Step succeeded for word 'logic'.\n",
      "Step succeeded for word 'nation'.\n",
      "Step succeeded for word 'test'.\n",
      "##### Test: succes #####\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n###### Testing the index: ###### \")\n",
    "\n",
    "words = [\"logic\", \"nation\", \"test\"]\n",
    "noErrors = True\n",
    "for word in words:\n",
    "    idxDocContain = set(idx.getTfsForStem(word).keys())\n",
    "\n",
    "    parser.initFile(cacm_txt)\n",
    "    doc = parser.nextDocument()\n",
    "    parserDocContain = set()\n",
    "    while doc is not None:\n",
    "        docText = doc.getText()\n",
    "        docStems = stemmer.getTextRepresentation(docText).keys()\n",
    "        docId = int(doc.getId())\n",
    "        if word in docStems:\n",
    "            parserDocContain.add(docId)\n",
    "            if docId not in idxDocContain:\n",
    "                print(\"Doc #%d contains %s: “%s”\" % (docId, word, docText))\n",
    "        elif docId in idxDocContain:\n",
    "            print(\"Doc #%d should contain %s: “%s”\" % (docId, word, docText))\n",
    "\n",
    "        doc = parser.nextDocument()\n",
    "\n",
    "    if len(idxDocContain - parserDocContain) > 0 or len(parserDocContain - idxDocContain) > 0:\n",
    "        print(\"Step failed for word '%s'\" % word)\n",
    "        break\n",
    "    else:\n",
    "        print(\"Step succeeded for word '%s'.\" % word)\n",
    "    \n",
    "if noErrors:\n",
    "    print(\"##### Test: succes #####\")\n",
    "else:\n",
    "    print(\"##### Test: fail #####\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Links from the 1st article: [43, 53, 91, 100, 123, 164, 165, 196, 205, 210, 214, 324, 398, 410, 642, 669, 1273, 1883, 1982, 3184]\n",
      "(Should be [43, 53, 91, ..., 1883, 1982, 3184])\n",
      "\n",
      "Links from the 2nd article: []\n",
      "(Should be empty)\n",
      "\n",
      "Links to the 1st article: [43, 53, 91, 100, 123, 164, 165, 196, 205, 210, 214, 324, 398, 410, 642, 669, 1273, 1883, 1982, 3184]\n",
      "(Should be [43, 53, 91, ..., 1883, 1982, 3184])\n",
      "\n",
      "Links to the 43rd article: [1, 205]\n",
      "(Should be [1, 205])\n"
     ]
    }
   ],
   "source": [
    "print(\"Links from the 1st article:\", sorted(idx.getSuccNodes('1')))\n",
    "print(\"(Should be [43, 53, 91, ..., 1883, 1982, 3184])\")\n",
    "print(\"\\nLinks from the 2nd article:\", idx.getSuccNodes('2'))\n",
    "print(\"(Should be empty)\")\n",
    "\n",
    "print(\"\\nLinks to the 1st article:\", sorted(idx.getPrevNodes('1')))\n",
    "print(\"(Should be [43, 53, 91, ..., 1883, 1982, 3184])\")\n",
    "print(\"\\nLinks to the 43rd article:\", sorted(idx.getPrevNodes('43')))\n",
    "print(\"(Should be [1, 205])\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Choosing what to test:\n",
    "test_binaryweighter = True\n",
    "test_tfidfweighter = True\n",
    "test_queryparser = True\n",
    "test_PRrecallmeasure = True\n",
    "test_averageprecision = True\n",
    "test_eval_ir_model = True\n",
    "test_unigram_model = True\n",
    "test_okapi = True\n",
    "gridsearch_language = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Testing the weighters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "#####Testing BinaryWeighter: #####\n",
      "bw.getDocWeightsForDoc( 3204 ): {'univers': 1, 'requir': 1, 'develop': 1, 'santa': 1, 'access': 1, 'numer': 1, 'assist': 1, 'lisp': 1, 'step': 1, 'programm': 1, 'consol': 1, 'line': 1, 'korsvold': 1, '1': 1, 'easi': 1, 'short': 1, 'design': 1, 'mathemat': 1, 'california': 1, 'program': 4, '5': 1, 'obtain': 1, 'written': 1, 'monica': 1, 'algebra': 1, 'chosen': 1, 'time': 2, 'debugg': 1, 'te': 1, 'teletyp': 1, 'remot': 1, 'share': 1, 'automat': 1, 'goal': 1, 'corpor': 1, 'comput': 2, 'result': 1, 'stanford': 1, '32': 1, 'compil': 1}\n",
      "\n",
      "bw.getWeightsForQuery(' Parallel languages; languages for parallel computation'): {'parallel': 1, 'comput': 1, 'languag': 1}\n",
      "\n",
      "#####Testing Vectoriel with BinaryWeighter: #####\n",
      "Top 10 documents for the previous query:\n",
      "Should include some of the following docs: 1043, 1188, 1306, \n",
      "    1358, 1396, 1491, 1923, 2246, 2316, 2527, 2699, 2710, 2715, 2716, \n",
      "    2906, 2923, 2956, 3073, 3150, \n",
      "[('2785', 0.6196773353931867), ('2685', 0.60522753266880247), ('1306', 0.58950634474656327), ('1262', 0.55708601453115558), ('1366', 0.53978144024077934), ('1471', 0.53333333333333333), ('1496', 0.48686449556014766), ('1659', 0.47628967220784024), ('93', 0.47140452079103173), ('830', 0.47140452079103173)]\n"
     ]
    }
   ],
   "source": [
    "query = stemmer.getTextRepresentation(\" Parallel languages; languages for parallel computation\")\n",
    "\n",
    "if test_binaryweighter:\n",
    "    print(\"\\n#####Testing BinaryWeighter: #####\")\n",
    "\n",
    "    bw = modeles.BinaryWeighter(idx)\n",
    "    print(\"bw.getDocWeightsForDoc(\",docId,\"):\", \n",
    "        bw.getDocWeightsForDoc(docId))\n",
    "    print(\"\\nbw.getWeightsForQuery(' Parallel languages; languages for parallel computation'):\", \n",
    "        bw.getWeightsForQuery(query))\n",
    "\n",
    "    print(\"\\n#####Testing Vectoriel with BinaryWeighter: #####\")\n",
    "    vect = modeles.Vectoriel(idx, bw)\n",
    "    print(\"Top 10 documents for the previous query:\")\n",
    "    print(\"\"\"Should include some of the following docs: 1043, 1188, 1306, \n",
    "    1358, 1396, 1491, 1923, 2246, 2316, 2527, 2699, 2710, 2715, 2716, \n",
    "    2906, 2923, 2956, 3073, 3150, \"\"\")\n",
    "    print(vect.getRanking(query)[:10])\n",
    "else:\n",
    "    print(\"\\n##### Skipping the test of BinaryWeighter #####\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "###### Testing TfidfWeighter: ###### \n",
      "tfidfWeighter.getDocWeightsForDoc( 3204 ): {'univers': 1, 'requir': 1, 'develop': 1, 'santa': 1, 'access': 1, 'numer': 1, 'assist': 1, 'lisp': 1, 'step': 1, 'programm': 1, 'consol': 1, 'line': 1, 'korsvold': 1, '1': 1, 'easi': 1, 'short': 1, 'design': 1, 'mathemat': 1, 'california': 1, 'program': 4, '5': 1, 'obtain': 1, 'written': 1, 'monica': 1, 'algebra': 1, 'chosen': 1, 'time': 2, 'debugg': 1, 'te': 1, 'teletyp': 1, 'remot': 1, 'share': 1, 'automat': 1, 'goal': 1, 'corpor': 1, 'comput': 2, 'result': 1, 'stanford': 1, '32': 1, 'compil': 1}\n",
      "\n",
      "tfidfWeighter.getWeightsForQuery(query): {'parallel': 3.7154464814986583, 'comput': 1.2477816381451639, 'languag': 2.063342122745655}\n",
      "\n",
      "###### Testing Vectoriel with TfidfWeighter: ###### \n",
      "Top 10 documents for the query:\n",
      "[('2785', 0.58402818133982293), ('1262', 0.55022124789918558), ('2685', 0.4705180276134806), ('141', 0.4194149275774155), ('2664', 0.41617615379853456), ('2973', 0.40635424019291577), ('1306', 0.39507197568948604), ('1471', 0.3883464830933987), ('392', 0.37513611553650089), ('950', 0.37351311122657732)]\n"
     ]
    }
   ],
   "source": [
    "tfidfWeighter = modeles.TfidfWeighter(idx)\n",
    "vect = modeles.Vectoriel(idx, tfidfWeighter)\n",
    "\n",
    "if test_tfidfweighter:\n",
    "    print(\"\\n###### Testing TfidfWeighter: ###### \")\n",
    "    print(\"tfidfWeighter.getDocWeightsForDoc(\", docId, \"):\",\n",
    "         tfidfWeighter.getDocWeightsForDoc(docId))\n",
    "    print(\"\\ntfidfWeighter.getWeightsForQuery(query):\",\n",
    "         tfidfWeighter.getWeightsForQuery(query))\n",
    "    print(\"\\n###### Testing Vectoriel with TfidfWeighter: ###### \")\n",
    "    print(\"Top 10 documents for the query:\")\n",
    "    print(vect.getRanking(query)[:10])\n",
    "else:\n",
    "    print(\"\\n##### Skipping the test of TfidfWeighter #####\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Testing the QueryParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if test_queryparser:\n",
    "    print(\"\\n###### Testing QueryParserCACM: ###### \")\n",
    "    qp = QueryParserCACM(cacm_qry, cacm_rel)\n",
    "    query = qp.nextQuery()\n",
    "    print(\"Searching for query #%d:\" % queryId)\n",
    "    while query is not None and query.getID() != str(queryId):\n",
    "        #print(query)\n",
    "        #print(20*'-')\n",
    "        query = qp.nextQuery()\n",
    "\n",
    "    print(\"Query:\", query)\n",
    "    queryTxt = stemmer.getTextRepresentation(query.getText())\n",
    "    print(\"\\nCompute scores:\")\n",
    "    scores = vect.getRanking(queryTxt)\n",
    "    print(scores[:10])\n",
    "    print(\"Done.\")\n",
    "else:\n",
    "    print(\"\\n##### Skipping the test of QueryParserCACM #####\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Testing precision/recall measure:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "if test_PRrecallmeasure:\n",
    "    print(\"\\n###### Testing evaluation.PrecisionRecallMeasure: ###### \")\n",
    "    queryChosen = np.random.randint(1, 50, size=10)\n",
    "    queryChosen =[7, 10, 25]#, 14, 26, 27, 42, 43] # lots of relevant results\n",
    "    qp = QueryParserCACM(cacm_qry, cacm_rel)\n",
    "    query = qp.nextQuery()\n",
    "    print(\"Searching for query #\", queryChosen)\n",
    "    while query is not None :\n",
    "        if int(query.getID()) in queryChosen:\n",
    "            print(\"Query:\", query)\n",
    "            queryTxt = stemmer.getTextRepresentation(query.getText())\n",
    "            print(\"Retrieve scores...\")\n",
    "            %time ranking = vect.getRanking(queryTxt)\n",
    "            print(\"Create PrecisionRecallMeasure object\")\n",
    "            irlist = evaluation.IRList(query, ranking)\n",
    "            precisRecall = evaluation.PrecisionRecallMeasure(irlist)\n",
    "            print(\"Evaluate the scores.\")\n",
    "            pr = precisRecall.eval(verbose=True, nbLevel=100) # (recall, precision)\n",
    "            precision = [p for r,p in pr]\n",
    "            recall = [r for r,p in pr]\n",
    "            plt.plot(recall, precision)\n",
    "            plt.title(\"Precision-Recall for query #\"+query.getID())\n",
    "            plt.show()\n",
    "            print(20*'-')\n",
    "\n",
    "\n",
    "        query = qp.nextQuery()\n",
    "    print(\"Done.\")\n",
    "else:\n",
    "    print(\"\\n##### Skipping the test of PrecisionRecallMeasure #####\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Testing average precision:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "if test_averageprecision:\n",
    "    print(\"\\n###### Testing evaluation.AveragePrecision: ###### \")\n",
    "    queryChosen = np.random.randint(1, 50, size=10)\n",
    "    #queryChosen =[7, 10, 14, 29]#, 25, 26, 27, 42, 43] # lots of relevant results\n",
    "    qp = QueryParserCACM(cacm_qry, cacm_rel)\n",
    "    query = qp.nextQuery()\n",
    "    print(\"Searching for query #\",queryChosen, \"\\n\")\n",
    "    while query is not None :\n",
    "        if int(query.getID()) in queryChosen:\n",
    "            print(\"Query:\", query)\n",
    "            queryTxt = stemmer.getTextRepresentation(query.getText())\n",
    "            print(\"Retrieve scores...\")\n",
    "            scores = vect.getRanking(queryTxt)\n",
    "            irlist = evaluation.IRList(query, scores)\n",
    "            average_measure = evaluation.AveragePrecision(irlist)\n",
    "            print(\"Evaluate the scores.\")\n",
    "            average_prec = average_measure.eval(verbose=True)\n",
    "            print(\"Average precision: %f\" % average_prec)\n",
    "            print(20*'-')\n",
    "\n",
    "        query = qp.nextQuery()\n",
    "    print(\"Done.\")\n",
    "else:\n",
    "    print(\"\\n##### Skipping the test of AveragePrecision #####\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Testing EvalIRModel:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "###### Testing evaluation.EvalIRModel: ###### \n",
      "Searching for queries # [16 29 27 21 29 47 11 10 28 30  3 18 43 46 20 19 25 19 19  4 22 36 19 37 37\n",
      " 21 34 23 21 22 41  6  1 13 29 10  6 42 36 41 24  7 30 46 33 11 47 29 28 12] ...\n",
      "Found queries\n",
      "Calling eval()...\n",
      "{('vectoriel tfidf', 'averagePrecision'): (0.27174578704981617, 0.22551191692163475), ('vectoriel bw', 'averagePrecision'): (0.10977927931553003, 0.17594830484315005)}\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "if test_eval_ir_model:\n",
    "    print(\"\\n###### Testing evaluation.EvalIRModel: ###### \")\n",
    "    queryChosen = np.random.randint(1, 50, size=50)\n",
    "    #queryChosen =[7, 10, 14]#, 25, 26, 27, 42, 43] # lots of relevant results\n",
    "    queries = []\n",
    "    qp = QueryParserCACM(cacm_qry, cacm_rel)\n",
    "    query = qp.nextQuery()\n",
    "    print(\"Searching for queries #\",queryChosen, \"...\")\n",
    "    while query is not None :\n",
    "        if int(query.getID()) in queryChosen:\n",
    "            queries.append(query)\n",
    "        query = qp.nextQuery()\n",
    "    print(\"Found queries\")\n",
    "    irmodels = {\"vectoriel bw\": modeles.Vectoriel(idx, bw),\n",
    "                \"vectoriel tfidf\": modeles.Vectoriel(idx, tfidfWeighter)}\n",
    "    measures = {\"averagePrecision\": evaluation.AveragePrecision}\n",
    "    eval_model = evaluation.EvalIRModel(queries, irmodels, measures)\n",
    "    print(\"Calling eval()...\")\n",
    "    print(eval_model.eval(verbose=False))\n",
    "    print(\"Done.\")\n",
    "else:\n",
    "    print(\"\\n##### Skipping the test of EvalIRModel #####\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Testing the unigram language model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "if test_unigram_model:\n",
    "    print(\"\\n###### Testing Unigram language: ###### \")\n",
    "    queryChosen = np.random.randint(1, 50, size=3)\n",
    "    #queryChosen =[7, 10, 14]#, 25, 26, 27, 42, 43] # lots of relevant results\n",
    "    queries = []\n",
    "    relevants = {} #dict of {query id : list of relevant doc id}\n",
    "    qp = QueryParserCACM(cacm_qry, cacm_rel)\n",
    "    query = qp.nextQuery()\n",
    "    print(\"Searching for queries #\",queryChosen, \"...\")\n",
    "    while query is not None :\n",
    "        if int(query.getID()) in queryChosen:\n",
    "            queries.append(query)\n",
    "            relevants[query.getID()] = list(query.getRelevants().keys())\n",
    "            #print(query)\n",
    "        query = qp.nextQuery()\n",
    "    print(\"Found queries\")\n",
    "    model = modeles.UnigramLanguage(idx, 0.8)\n",
    "    for q, (q_id, relev) in zip(queries, relevants.items()):\n",
    "        print(q)\n",
    "        print(\"Scores for 3 relevant docs:\")\n",
    "        for doc_id in np.random.choice(relev, size=3):\n",
    "            print(doc_id, model.score(stemmer.getTextRepresentation(q.getText()), doc_id))\n",
    "        print(\"Scores for 3 random docs:\")\n",
    "        for doc_id in np.random.choice(idx.getDocsID(), size=3):\n",
    "            if doc_id in relev:\n",
    "                print(\"Doc #\", doc_id, \"Relevant doc\")\n",
    "            else:\n",
    "                stems = ','.join(idx.getTfsForDoc(doc_id).keys())\n",
    "                print(\"Irrelevant doc #\", doc_id, stems)\n",
    "                print(\"Score for this doc:\", \n",
    "                      model.score(stemmer.getTextRepresentation(q.getText()), doc_id))\n",
    "        print(20*'-')\n",
    "    print(\"Done.\")\n",
    "else:\n",
    "    print(\"\\n##### Skipping the test of LanguageModel #####\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "##  Testing Okapi language model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "if test_okapi:\n",
    "    print(\"\\n###### Testing Okapi: ###### \")\n",
    "    queryChosen = np.random.randint(1, 50, size=3)\n",
    "    #queryChosen =[7, 10, 14]#, 25, 26, 27, 42, 43] # lots of relevant results\n",
    "    queries = []\n",
    "    relevants = {} #dict of {query id : list of relevant doc id}\n",
    "    qp = QueryParserCACM(cacm_qry, cacm_rel)\n",
    "    query = qp.nextQuery()\n",
    "    print(\"Searching for queries #\",queryChosen, \"...\")\n",
    "    while query is not None :\n",
    "        if int(query.getID()) in queryChosen:\n",
    "            queries.append(query)\n",
    "            relevants[query.getID()] = list(query.getRelevants().keys())\n",
    "            #print(query)\n",
    "        query = qp.nextQuery()\n",
    "    print(\"Found queries\")\n",
    "    \n",
    "    okapi = modeles.Okapi(idx, k=1, b=1)\n",
    "    for q, (q_id, relev) in zip(queries, relevants.items()):\n",
    "        print(q)\n",
    "        print(\"Scores for 3 relevant docs:\")\n",
    "        for doc_id in np.random.choice(relev, size=3):\n",
    "            stems = ','.join(idx.getTfsForDoc(doc_id).keys())\n",
    "            print(\"Relevant doc #\", doc_id, stems)\n",
    "            print(\"Score:\", \n",
    "                  okapi.score(stemmer.getTextRepresentation(q.getText()), doc_id, verbose=True))\n",
    "        print(\"Scores for 3 random docs:\")\n",
    "        for doc_id in np.random.choice(idx.getDocsID(), size=3):\n",
    "            if doc_id in relev:\n",
    "                print(\"Doc #\", doc_id, \"Relevant doc\")\n",
    "            else:\n",
    "                stems = ','.join(idx.getTfsForDoc(doc_id).keys())\n",
    "                print(\"Irrelevant doc #\", doc_id, stems)\n",
    "                print(\"Score for this doc:\", \n",
    "                      okapi.score(stemmer.getTextRepresentation(q.getText()), doc_id, verbose=True))\n",
    "        print(20*'-')\n",
    "    print(\"Done.\")\n",
    "else:\n",
    "    print(\"\\n##### Skipping the test of Okapi #####\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "print(\"\\n###### Testing Okapi & AveragePrecision: ###### \")\n",
    "for query in queries:\n",
    "    print(\"Query:\", query)\n",
    "    queryTxt = stemmer.getTextRepresentation(query.getText())\n",
    "    print(\"Retrieve scores...\")\n",
    "    scores = okapi.getRanking(queryTxt)\n",
    "    irlist = evaluation.IRList(query, scores)\n",
    "    average_measure = evaluation.AveragePrecision(irlist)\n",
    "    print(\"Evaluate the scores.\")\n",
    "    average_prec = average_measure.eval(verbose=True)\n",
    "    print(\"Average precision: %f\" % average_prec)\n",
    "    print(20*'-')\n",
    "\n",
    "print(\"Done.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Comparison of language models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import itertools \n",
    "\n",
    "# Searching queries:\n",
    "queries = []\n",
    "qp = QueryParserCACM(cacm_qry, cacm_rel)\n",
    "query = qp.nextQuery()\n",
    "print(\"Searching for queries #\",queryChosen, \"...\")\n",
    "while query is not None :\n",
    "    if int(query.getID()) in queryChosen:\n",
    "        queries.append(query)\n",
    "    query = qp.nextQuery()\n",
    "print(\"Found all queries\")\n",
    "\n",
    "q_train, q_test = train_test_split(queries)\n",
    "\n",
    "# Train models, find best parameters.\n",
    "\n",
    "def dict_combinations(dic):\n",
    "    keys = dic.keys()\n",
    "    #print(keys)\n",
    "    values = [dic[key] for key in keys]\n",
    "    #print(\"values:\", list(values), \".\")\n",
    "    #for combination in itertools.product(*values):\n",
    "        #print(combination)\n",
    "    combinations = [dict(zip(keys, combination)) for combination in itertools.product(*values)]\n",
    "    return combinations\n",
    "\n",
    "def gridsearch(model_class, param_grid, queries, measure_object, verbose=False):\n",
    "    \"\"\"\n",
    "    :param model_class: modeles.Vectoriel for instance (the class, not an instance)\n",
    "    :param param_grid: dict of {string:iterable}\n",
    "    :param queries: list of Query objects\n",
    "    :param measure_class: evaluation.AveragePrecision() for instance\n",
    "    \"\"\"\n",
    "    params = []\n",
    "    irmodels = {}\n",
    "    for i, comb in enumerate(dict_combinations(param_grid)):\n",
    "        params.append(comb)\n",
    "        irmodels[i] = model_class(**comb)\n",
    "    eval_models = evaluation.EvalIRModel(queries, irmodels, {'measure':measure_object})\n",
    "    if verbose:\n",
    "        print(\"Calling eval()\")\n",
    "        scores = eval_models.eval(verbose=verbose)\n",
    "        for k,v in scores.items():\n",
    "            print(params[k[0]])\n",
    "            print(\"--->\", v[0])\n",
    "    else:\n",
    "        scores = eval_models.eval()\n",
    "    best_irmodel = max(scores.keys(), key=(lambda key: scores[key][0]))[0]\n",
    "    return params[best_irmodel]\n",
    "\n",
    "if gridsearch_language:\n",
    "    unigram_params_grid = {'index':[idx], 'regularization':np.linspace(0,1, 20)}\n",
    "\n",
    "    #c = dict_combinations(params_grid)\n",
    "    #print(\"comb: \",c)\n",
    "\n",
    "    best_unigram_params = gridsearch(modeles.UnigramLanguage, unigram_params_grid, \n",
    "                     q_train, evaluation.AveragePrecision, verbose=1)\n",
    "else:\n",
    "    best_unigram_params = {'index': idx, 'regularization': 0.36842105263157893}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "if gridsearch_language:\n",
    "    okapi_params_grid = {'index':[idx], 'k':np.linspace(1, 2, 10), 'b':np.linspace(0,2,10)}\n",
    "    best_okapi_params = gridsearch(modeles.Okapi, okapi_params_grid, \n",
    "                 q_train, evaluation.AveragePrecision, verbose=1)\n",
    "\n",
    "else:\n",
    "    best_okapi_params = {'index': idx, 'k': 2.0, 'b': 0.5}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Execution on the test set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "print(\"best unigram params:\", best_unigram_params)\n",
    "print(\"best okapi params:\", best_okapi_params)\n",
    "\n",
    "irmodels = {'unigram':modeles.UnigramLanguage(**best_unigram_params),\n",
    "            'okapi':modeles.Okapi(**best_okapi_params)}\n",
    "\n",
    "eval_models = evaluation.EvalIRModel(q_test, irmodels, {'measure':evaluation.AveragePrecision})\n",
    "scores = eval_models.eval(verbose=True)\n",
    "print(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
